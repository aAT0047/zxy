{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0f72b0-b2ae-4e76-b10c-a0297b95c01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.stats as stats\n",
    "\n",
    "# Given values\n",
    "mean_production_time = 3  # days\n",
    "std_deviation = 0.5  # days\n",
    "delivery_time = 6  # days\n",
    "\n",
    "# Calculate Z-score for 6 days\n",
    "z_score = (delivery_time - mean_production_time) / std_deviation\n",
    "\n",
    "# Calculate the probability of orders being delivered within 6 days\n",
    "prob_within_6_days = stats.norm.cdf(z_score)\n",
    "\n",
    "# The probability of orders being delivered beyond 6 days\n",
    "prob_beyond_6_days = 1 - prob_within_6_days\n",
    "print('a',prob_beyond_6_days)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35acb2f9-aafc-4055-b220-a21bb4d62daa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Step 1: Prepare data for each hour\n",
    "hourly_processing_rate_data = []\n",
    "\n",
    "# For 7:00 am to 10:00 am (20 workers, 12,000 mails per hour)\n",
    "for hour in range(7, 10):\n",
    "    hourly_processing_rate_data.append([hour, 20, 12000])\n",
    "\n",
    "# For 10:00 am to 3:00 pm (60 workers, 36,000 mails per hour)\n",
    "for hour in range(10, 15):\n",
    "    hourly_processing_rate_data.append([hour, 60, 36000])\n",
    "\n",
    "# For 3:00 pm to 7:00 pm (40 workers, 24,000 mails per hour)\n",
    "for hour in range(15, 19):\n",
    "    hourly_processing_rate_data.append([hour, 40, 24000])\n",
    "\n",
    "# Step 2: Create DataFrame for Processing Rates\n",
    "df_hourly_processing_rate = pd.DataFrame(hourly_processing_rate_data, columns=[\"Time (Hour)\", \"Number of Workers\", \"Processing Rate (per hour)\"])\n",
    "\n",
    "# Step 3: Prepare data for each hour's mail arrival rate\n",
    "hourly_mail_arrival_data = []\n",
    "\n",
    "# For 7:00 am to 10:00 am (24,000 mails per hour)\n",
    "for hour in range(7, 10):\n",
    "    hourly_mail_arrival_data.append([hour, 24000])\n",
    "\n",
    "# For 10:00 am to 3:00 pm (34,000 mails per hour)\n",
    "for hour in range(10, 15):\n",
    "    hourly_mail_arrival_data.append([hour, 34000])\n",
    "\n",
    "# For 3:00 pm to 7:00 pm (11,500 mails per hour)\n",
    "for hour in range(15, 19):\n",
    "    hourly_mail_arrival_data.append([hour, 11500])\n",
    "\n",
    "# Step 4: Create DataFrame for Mail Arrival Rates\n",
    "df_hourly_mail_arrival = pd.DataFrame(hourly_mail_arrival_data, columns=[\"Time (Hour)\", \"Mail Arrival Rate (per hour)\"])\n",
    "\n",
    "# Step 5: Combine the data into a single table\n",
    "combined_data = {\n",
    "    \"Time (Hour)\": list(range(7, 19)),\n",
    "    \"Mail Arrival Rate (per hour)\": [24000, 24000, 24000, 34000, 34000, 34000, 34000, 34000, 11500, 11500, 11500, 11500],\n",
    "    \"Number of Workers\": [20, 20, 20, 60, 60, 60, 60, 60, 40, 40, 40, 40],\n",
    "    \"Processing Rate (per hour)\": [12000, 12000, 12000, 36000, 36000, 36000, 36000, 36000, 24000, 24000, 24000, 24000]\n",
    "}\n",
    "\n",
    "# Step 6: Create a DataFrame with all combined data\n",
    "df_combined = pd.DataFrame(combined_data)\n",
    "\n",
    "# Step 7: Calculate the difference between Mail Arrival and Processing Rate\n",
    "df_combined['Difference (Mail Arrival - Processing Rate)'] = df_combined['Mail Arrival Rate (per hour)'] - df_combined['Processing Rate (per hour)']\n",
    "\n",
    "# Step 8: Calculate the cumulative difference (unprocessed mail)\n",
    "df_combined['Cumulative Difference'] = df_combined['Difference (Mail Arrival - Processing Rate)'].cumsum()\n",
    "\n",
    "# Step 9: Adjust for cases where unprocessed mail becomes zero\n",
    "df_combined['Cumulative Difference Adjusted'] = df_combined['Cumulative Difference'].clip(lower=0)  # Set negative values to zero\n",
    "\n",
    "# Recalculate the total unprocessed mail considering the adjusted values\n",
    "total_unprocessed_mail_adjusted = df_combined['Cumulative Difference Adjusted'].sum()\n",
    "\n",
    "# Calculate the average unprocessed mail (b) by dividing total unprocessed mail by the number of time periods\n",
    "total_hours = len(df_combined)\n",
    "average_unprocessed_mail_adjusted = total_unprocessed_mail_adjusted / total_hours\n",
    "\n",
    "# Step 10: Calculate the average waiting time for a mail (c) using Little's Law\n",
    "total_mail_arrival = df_combined['Mail Arrival Rate (per hour)'].sum()\n",
    "average_mail_arrival_rate = total_mail_arrival / total_hours\n",
    "average_waiting_time = average_unprocessed_mail_adjusted / average_mail_arrival_rate\n",
    "\n",
    "# Display combined data table\n",
    "import IPython.display as display\n",
    "display.display(df_combined)\n",
    "\n",
    "# Step 11: Plot the cumulative difference as an accumulated mail chart (representing unprocessed mail over time)\n",
    "plt.plot(df_combined['Time (Hour)'], df_combined['Cumulative Difference'], marker='o', linestyle='-', color='b')\n",
    "plt.title('Accumulated Unprocessed Mail Over Time')\n",
    "plt.xlabel('Time of Day (Hour)')\n",
    "plt.ylabel('Accumulated Unprocessed Mail')\n",
    "plt.grid(True)\n",
    "plt.xticks(df_combined['Time (Hour)'])\n",
    "plt.show()\n",
    "\n",
    "# Output the results for b and c\n",
    "average_unprocessed_mail_adjusted, average_waiting_time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64b2ad94-9ae4-4b66-a670-1ce65f885f07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# 过去60天的需求数据\n",
    "demand_data = np.array([8, 5, 12, 10, 3, 9, 9, 10, 9, 9, 12, 10, 10, 9, 11, 8, 11, 5, 11, 8, 7, 4, 7, 9, 7, 8, 9, \n",
    "                       4, 9, 12, 13, 12, 15, 12, 8, 7, 10, 8, 14, 13, 12, 14, 13, 9, 7, 10, 8, 14, 8, 10, 6, 8, \n",
    "                       10, 8, 8, 9, 9, 10, 10, 8, 9, 10, 9, 10, 9, 9])\n",
    "\n",
    "# 计算过去需求小于9的天数\n",
    "shortage_days = np.sum(demand_data < 9)\n",
    "\n",
    "# 计算缺货的概率\n",
    "probability_stockout = shortage_days / len(demand_data)\n",
    "\n",
    "# 输出缺货概率\n",
    "print('a',probability_stockout)\n",
    "from scipy.stats import poisson\n",
    "\n",
    "# 计算需求的平均值\n",
    "lambda_ = np.mean(demand_data)\n",
    "\n",
    "# 使用泊松分布计算需求小于等于9的概率\n",
    "probability_stockout_poisson = poisson.cdf(8, lambda_)  # P(X <= 8)\n",
    "\n",
    "# 输出缺货概率\n",
    "print('b',probability_stockout_poisson)\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "# 计算需求的均值和标准差\n",
    "mean_demand = np.mean(demand_data)\n",
    "std_demand = np.std(demand_data)\n",
    "\n",
    "# 使用正态分布计算需求小于等于9的概率\n",
    "probability_stockout_normal = norm.cdf(8, mean_demand, std_demand)  # P(X <= 8)\n",
    "\n",
    "# 输出缺货概率\n",
    "print('c',probability_stockout_normal)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ba77133-7897-408c-93e2-821b5766a9b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.api import ExponentialSmoothing\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from scipy.stats import norm\n",
    "\n",
    "# 销售数据（过去12周）\n",
    "sales = np.array([810, 991, 1067, 942, 977, 729, 1052, 941, 804, 847, 754, 915])\n",
    "\n",
    "# 将销售数据转换为pandas Series\n",
    "sales_series = pd.Series(sales)\n",
    "\n",
    "# 使用指数平滑法进行预测\n",
    "# model = ExponentialSmoothing(sales_series, trend='add', seasonal=None, damped=False).fit()\n",
    "model = ExponentialSmoothing(sales_series, trend='add', seasonal=None, damped_trend=False).fit()\n",
    "\n",
    "# 预测未来4周的销售量\n",
    "forecast_next_4_weeks = model.forecast(steps=4)\n",
    "\n",
    "# 计算均方误差（MSE）\n",
    "mse = mean_squared_error(sales[1:], model.fittedvalues[1:])\n",
    "\n",
    "# 计算预测的标准误差（SE）\n",
    "se = np.sqrt(mse)\n",
    "\n",
    "# 计算预测销售的均值\n",
    "forecast_mean = forecast_next_4_weeks.mean()\n",
    "\n",
    "# 计算短缺的Z值\n",
    "z_score = (800 - forecast_mean) / se\n",
    "\n",
    "# 计算短缺概率\n",
    "shortage_probability = 1 - norm.cdf(z_score)\n",
    "\n",
    "# 绘制实际销售值和预测销售值的曲线\n",
    "weeks = np.arange(1, 13)\n",
    "forecast_weeks = np.arange(13, 17)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(weeks, sales, label='Actual Sales', marker='o', color='b')\n",
    "plt.plot(weeks, model.fittedvalues, label='Fitted Values', color='g')\n",
    "plt.plot(forecast_weeks, forecast_next_4_weeks, label='Forecast', marker='o', linestyle='--', color='r')\n",
    "\n",
    "plt.title(\"Sales Forecast using Exponential Smoothing\")\n",
    "plt.xlabel('Weeks')\n",
    "plt.ylabel('Sales (Units)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 输出MSE和短缺概率\n",
    "mse, shortage_probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15456ed1-4290-4b00-bc50-96d66b91273e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.holtwinters import Holt\n",
    "\n",
    "# Sales data for the last 16 weeks\n",
    "sales = np.array([11797, 6724, 15346, 12825, 15939, 9997, 13671, 21994, 17774, 14587, \n",
    "                 11190, 18083, 25320, 15129, 17092, 27652])\n",
    "\n",
    "# Holt's method parameters\n",
    "alpha = 0.7\n",
    "beta = 0.1\n",
    "\n",
    "# Convert sales data to a pandas Series for the model\n",
    "sales_series = pd.Series(sales)\n",
    "\n",
    "# Apply Holt's method with given alpha and beta\n",
    "model = Holt(sales_series).fit(smoothing_level=alpha, smoothing_trend=beta)\n",
    "\n",
    "# Forecast the next 2 weeks\n",
    "forecast_next_2_weeks_holt = model.forecast(steps=2)\n",
    "\n",
    "# Weeks for actual sales and forecasted sales\n",
    "weeks = np.arange(1, 17)\n",
    "forecast_weeks = np.arange(17, 19)\n",
    "\n",
    "# Plot the sales and forecasts\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(weeks, sales, label='Actual Sales', marker='o', color='b')\n",
    "plt.plot(weeks, model.fittedvalues, label='Fitted Values', color='g')\n",
    "plt.plot(forecast_weeks, forecast_next_2_weeks_holt, label='Forecast', marker='o', linestyle='--', color='r')\n",
    "\n",
    "plt.title(\"Sales Forecast using Holt's Method (holt package)\")\n",
    "plt.xlabel('Weeks')\n",
    "plt.ylabel('Sales (Boxes)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Output forecast values for the next 2 weeks\n",
    "print(forecast_next_2_weeks_holt)\n",
    "\n",
    "# Part (b) - Find the best alpha and beta that minimize MSE\n",
    "best_mse = float('inf')\n",
    "best_alpha = best_beta = 0\n",
    "alpha_values = np.linspace(0.1, 1.0, 10)\n",
    "beta_values = np.linspace(0.1, 1.0, 10)\n",
    "\n",
    "# Test all combinations of alpha and beta\n",
    "for alpha in alpha_values:\n",
    "    for beta in beta_values:\n",
    "        holt_model = Holt(sales).fit(smoothing_level=alpha, smoothing_trend=beta)\n",
    "        forecast = holt_model.forecast(steps=len(sales)-1)\n",
    "        mse = mean_squared_error(sales[1:], forecast)\n",
    "        if mse < best_mse:\n",
    "            best_mse = mse\n",
    "            best_alpha = alpha\n",
    "            best_beta = beta\n",
    "\n",
    "print(\"\\nPart (b) Optimal alpha and beta:\")\n",
    "print(f\"Optimal alpha: {best_alpha}\")\n",
    "print(f\"Optimal beta: {best_beta}\")\n",
    "print(f\"Minimum MSE: {best_mse:.2f}\")\n",
    "\n",
    "# Part (c) - Use the method in part (b) to develop point forecasts and 90% prediction interval forecasts\n",
    "holt_model_opt = Holt(sales).fit(smoothing_level=best_alpha, smoothing_trend=best_beta)\n",
    "forecast_2_weeks_opt = holt_model_opt.forecast(steps=2)\n",
    "\n",
    "# Calculate the standard error of the forecast\n",
    "forecast_residuals = sales - holt_model_opt.fittedvalues\n",
    "se = np.std(forecast_residuals)  # Standard deviation of residuals\n",
    "\n",
    "# Calculate prediction intervals for 90% confidence level\n",
    "z_score = 1.645\n",
    "se_17 = se * np.sqrt(1 + 1)\n",
    "se_18 = se * np.sqrt(1 + 2)\n",
    "\n",
    "# Prediction intervals for week 17 and 18\n",
    "lower_17 = forecast_2_weeks_opt[0] - z_score * se_17\n",
    "upper_17 = forecast_2_weeks_opt[0] + z_score * se_17\n",
    "\n",
    "lower_18 = forecast_2_weeks_opt[1] - z_score * se_18\n",
    "upper_18 = forecast_2_weeks_opt[1] + z_score * se_18\n",
    "\n",
    "print(\"\\nPart (c) Point forecasts and 90% prediction intervals:\")\n",
    "print(f\"Week 17 forecast: {forecast_2_weeks_opt[0]:.2f} boxes\")\n",
    "print(f\"90% prediction interval for Week 17: [{lower_17:.2f}, {upper_17:.2f}] boxes\")\n",
    "print(f\"Week 18 forecast: {forecast_2_weeks_opt[1]:.2f} boxes\")\n",
    "print(f\"90% prediction interval for Week 18: [{lower_18:.2f}, {upper_18:.2f}] boxes\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75dec539-62e8-4754-94f1-0820d5d2a2b8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8a40d2-c540-41c0-8417-3a8ab67367ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "shortage_probability\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156f4950-e582-4b67-bef6-118a6232eb42",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
